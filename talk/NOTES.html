<!DOCTYPE html>
	<!--
	Google HTML5 slide template

Authors: Luke Mah?? (code)
Marcin Wichary (code and design)

Dominic Mazzoni (browser compatibility)
Charles Chen (ChromeVox support)

URL: http://code.google.com/p/html5slides/
	-->
	
<html>
<head>
	
	<meta charset='utf-8'>
	<title>Bayesian Nonparametrics</title>
  <meta name="description" content="Bayesian Nonparametrics">
  <meta name="author" content="">
  <meta name="generator" content="slidify" />
	
	<!-- LOAD STYLE SHEETS -->
	<link rel="stylesheet" href="/Library/Frameworks/R.framework/Versions/2.15/Resources/library/slidify/libraries/html5slides/default/styles.css">
	<link rel="stylesheet" href="/Library/Frameworks/R.framework/Versions/2.15/Resources/library/slidify/libraries/html5slides/default/uulm.css">
	<link rel="stylesheet" href="/Library/Frameworks/R.framework/Versions/2.15/Resources/library/slidify/libraries/highlight.js/styles/github.css">
  <!-- LOAD CUSTOM CSS -->
  <link rel="stylesheet" href="assets/stylesheets/custom.css">

    
</head>
<body style='display: none'>
	<section class='slides layout-regular template-regular'>
	  <article class = "" id = "slide-1"> 
	    <h1>Bayesian Nonparametrics</h1>


<h3>An Introduction using R</h3>

<h3>John Myles White</h3>

    </article>
	  <article class = "" id = "slide-2"> 
	    <h3>Overview</h3>


<ul>
<li>Why use nonparametric statistics?</li>
</ul>

    </article>
	  <article class = "" id = "slide-3"> 
	    <h3>Overview</h3>


<ul>
<li>Why use nonparametric statistics?</li>
<li>Why use Bayesian statistics?</li>
</ul>

    </article>
	  <article class = "" id = "slide-4"> 
	    <h3>Overview</h3>


<ul>
<li>Why use nonparametric statistics?</li>
<li>Why use Bayesian statistics?</li>
<li>Why use Bayesian nonparametrics?</li>
</ul>

    </article>
	  <article class = "" id = "slide-5"> 
	    <h3>Overview</h3>


<ul>
<li>Why use nonparametric statistics?</li>
<li>Why use Bayesian statistics?</li>
<li>Why use Bayesian nonparametrics?</li>
<li>Two example uses of Bayesian nonparametrics</li>
</ul>

    </article>
	  <article class = "" id = "slide-6"> 
	    <h3>What is Statistics?</h3>


<ul>
<li>Statistics is a mathematical theory that tells us how we <em>should</em> learn about the structure of the world around us based on our experiences</li>
</ul>

    </article>
	  <article class = "" id = "slide-7"> 
	    <h3>What is Statistics?</h3>


<ul>
<li>Statistics is a mathematical theory that tells us how we <em>should</em> learn about the structure of the world around us based on our experiences</li>
<li>Statistical theory decides which methods are <em>good</em> and which are <em>bad</em> by inventing a theoretical world in which we can precisely measure how well we are learning from our experiences</li>
</ul>

    </article>
	  <article class = "" id = "slide-8"> 
	    <h3>What is Statistics?</h3>


<ul>
<li>Statistics is a mathematical theory that tells us how we <em>should</em> learn about the structure of the world around us based on our experiences</li>
<li>Statistical theory decides which methods are <em>good</em> and which are <em>bad</em> by inventing a theoretical world in which we can precisely measure how well we are learning from our experiences</li>
<li>The practical value of statistical theory depends upon the match between the structure of this theoretical world and the real world</li>
</ul>

    </article>
	  <article class = "" id = "slide-9"> 
	    <h3>What is Statistics?</h3>


<ul>
<li>Statistics is a mathematical theory that tells us how we <em>should</em> learn about the structure of the world around us based on our experiences</li>
<li>Statistical theory decides which methods are <em>good</em> and which are <em>bad</em> by inventing a theoretical world in which we can precisely measure how well we are learning from our experiences</li>
<li>The practical value of statistical theory depends upon the match between the structure of this theoretical world and the real world</li>
<li>But we need powerful models if we want to describe the complex structures that actually occur in the real world</li>
</ul>

    </article>
	  <article class = "" id = "slide-10"> 
	    <h3>What is Nonparametric Statistics?</h3>


<ul>
<li>Nonparametric statistical models try to pull out the structure of the world from data, rather than push the structure of our theories onto data</li>
</ul>

    </article>
	  <article class = "" id = "slide-11"> 
	    <h3>What is Nonparametric Statistics?</h3>


<ul>
<li>Nonparametric statistical models try to pull out the structure of the world from data, rather than push the structure of our theories onto data</li>
<li>Nonparametric statistical models &quot;let the data speak&quot; by employing highly flexible functional forms</li>
</ul>

    </article>
	  <article class = "" id = "slide-12"> 
	    <h3>What is Nonparametric Statistics?</h3>


<ul>
<li>Nonparametric statistical models try to pull out the structure of the world from data, rather than push the structure of our theories onto data</li>
<li>Nonparametric statistical models &quot;let the data speak&quot; by employing highly flexible functional forms</li>
<li>Nonparametric statistical models typically have infinitely many parameters</li>
</ul>

    </article>
	  <article class = "" id = "slide-13"> 
	    <h3>What is Nonparametric Statistics?</h3>


<ul>
<li>Nonparametric statistical models try to pull out the structure of the world from data, rather than push the structure of our theories onto data</li>
<li>Nonparametric statistical models &quot;let the data speak&quot; by employing highly flexible functional forms</li>
<li>Nonparametric statistical models typically have infinitely many parameters</li>
<li>But only a finite number of parameters are used when analyzing any specific data set</li>
</ul>

    </article>
	  <article class = "" id = "slide-14"> 
	    <h3>Nonparametric Density Estimation</h3>


<ul>
<li>Many data sets don&#39;t look like out-of-the-box statistical distributions:</li>
</ul>

<p><img src="examples/marathon/men_hours.pdf" alt="Marathon-Times"></p>

    </article>
	  <article class = "" id = "slide-15"> 
	    <h3>Nonparametric Density Estimation</h3>


<ul>
<li>We want to estimate the true distribution of runners&#39; times from this bimodal sample of data</li>
<li>We have two standard nonparametric density estimation tools:

<ul>
<li>Histograms</li>
<li>Kernel Density Estimators (KDE&#39;s)</li>
</ul></li>
</ul>

    </article>
	  <article class = "" id = "slide-16"> 
	    <h3>Histograms are a Nonparametric Method</h3>


<pre><code>ggplot(subset(df, SampleSize == &quot;100&quot;),
       aes(x = X, fill = SampleSize, color = SampleSize)) +
  geom_histogram() +
  geom_line(data = true.density, aes(x = X, y = 100 * Y)) +
  opts(title = &quot;Density Estimation with Histograms and 100 Data Points&quot;)
ggsave(&quot;talk/examples/density_estimation/histogram_100.pdf&quot;,
       height = 7,
       width = 7)
</code></pre>

    </article>
	  <article class = "" id = "slide-17"> 
	    <h3>Histograms are a Nonparametric Method</h3>


<ul>
<li>If you have infinite data, a histogram will converge on the true distribution</li>
</ul>

<p><img src="examples/density_estimation/histogram_100.pdf" alt="Histogram100"></p>

    </article>
	  <article class = "" id = "slide-18"> 
	    <h3>Histograms are a Nonparametric Method</h3>


<ul>
<li>If you have infinite data, a histogram will converge on the true distribution</li>
</ul>

<p><img src="examples/density_estimation/histogram_1000.pdf" alt="Histogram1000"></p>

    </article>
	  <article class = "" id = "slide-19"> 
	    <h3>Histograms are a Nonparametric Method</h3>


<ul>
<li>If you have infinite data, a histogram will converge on the true distribution</li>
</ul>

<p><img src="examples/density_estimation/histogram_10000.pdf" alt="Histogram10000"></p>

    </article>
	  <article class = "" id = "slide-20"> 
	    <h3>Histograms are a Nonparametric Method</h3>


<ul>
<li>If you have infinite data, a histogram will converge on the true distribution</li>
</ul>

<p><img src="examples/density_estimation/histogram_50000.pdf" alt="Histogram50000"></p>

    </article>
	  <article class = "" id = "slide-21"> 
	    <h3>KDE&#39;s are a Nonparametric Method</h3>


<pre><code>ggplot(subset(df, SampleSize == &quot;100&quot;),
       aes(x = X, fill = SampleSize, color = SampleSize)) +
  geom_density() +
  geom_line(data = true.density, aes(x = X, y = Y)) +
  opts(title = &quot;Density Estimation with Histograms and 100 Data Points&quot;)
ggsave(&quot;talk/examples/density_estimation/density_100.pdf&quot;,
       height = 7,
       width = 7)
</code></pre>

    </article>
	  <article class = "" id = "slide-22"> 
	    <h3>KDE&#39;s are a Nonparametric Method</h3>


<ul>
<li>If you have infinite data, a KDE will converge on the true distribution</li>
</ul>

<p><img src="examples/density_estimation/density_100.pdf" alt="Density100"></p>

    </article>
	  <article class = "" id = "slide-23"> 
	    <h3>KDE&#39;s are a Nonparametric Method</h3>


<ul>
<li>If you have infinite data, a KDE will converge on the true distribution</li>
</ul>

<p><img src="examples/density_estimation/density_1000.pdf" alt="Density1000"></p>

    </article>
	  <article class = "" id = "slide-24"> 
	    <h3>KDE&#39;s are a Nonparametric Method</h3>


<ul>
<li>If you have infinite data, a KDE will converge on the true distribution</li>
</ul>

<p><img src="examples/density_estimation/density_10000.pdf" alt="Density10000"></p>

    </article>
	  <article class = "" id = "slide-25"> 
	    <h3>KDE&#39;s are a Nonparametric Method</h3>


<ul>
<li>If you have infinite data, a KDE will converge on the true distribution</li>
</ul>

<p><img src="examples/density_estimation/density_50000.pdf" alt="Density50000"></p>

    </article>
	  <article class = "" id = "slide-26"> 
	    <h3>KDE&#39;s &gt; Histograms</h3>


<ul>
<li>KDE&#39;s converge on the true distribution faster than histograms</li>
</ul>

<p>\[
O(n^{-\frac{4}{5}}) < O(n^{-\frac{2}{3}})
\]</p>

    </article>
	  <article class = "" id = "slide-27"> 
	    <h3>Nonparametric Regression</h3>


<ul>
<li>Many data sets have relationships between variables that don&#39;t look like straight lines</li>
<li>We need regression models that are more expressive than linear regression</li>
</ul>

<p><img src="examples/weather/weather_points.pdf" alt="Weather"></p>

    </article>
	  <article class = "" id = "slide-28"> 
	    <h3>Nonparametric Regression</h3>


<ul>
<li>We have several standard nonparametric regression tools:

<ul>
<li>Loess</li>
<li>Generalized Additive Models (GAM&#39;s)</li>
<li>k-Nearest Neighbors Regression (kNN)</li>
</ul></li>
</ul>

    </article>
	  <article class = "" id = "slide-29"> 
	    <h3>Loess is a Nonparametric Method</h3>


<pre><code>ggplot(df, aes(x = DayOfYear, y = Precipitation)) +
  geom_point() +
  geom_smooth(method = &quot;loess&quot;) +
  xlab(&quot;Day of Year&quot;) +
  ylab(&quot;Probability of Precipitation&quot;) +
  opts(title = &quot;Chance of Rain in Seattle over the Course of the Year&quot;)
ggsave(&quot;examples/weather/weather_loess.pdf&quot;,
       height = 7,
       width = 7)
</code></pre>

    </article>
	  <article class = "" id = "slide-30"> 
	    <h3>Loess is a Nonparametric Method</h3>


<ul>
<li>If you have infinite data, loess may converge on the true distribution</li>
</ul>

<p><img src="examples/weather/weather_loess.pdf" alt="Weather"></p>

    </article>
	  <article class = "" id = "slide-31"> 
	    <h3>What is Bayesian Statistics?</h3>


<ul>
<li>Bayesian statistical models are enriched by prior knowledge

<ul>
<li>Prior knowledge can be viewed as regularization</li>
</ul></li>
</ul>

    </article>
	  <article class = "" id = "slide-32"> 
	    <h3>What is Bayesian Statistics?</h3>


<ul>
<li>Bayesian statistical models are enriched by prior knowledge

<ul>
<li>Prior knowledge can be viewed as regularization</li>
</ul></li>
<li>Knowledge gained from experience is quantified using probability theory:

<ul>
<li>Inferences are distributions</li>
<li>Not point estimates</li>
<li>Not interval estimates</li>
</ul></li>
</ul>

    </article>
	  <article class = "" id = "slide-33"> 
	    <h3>Why Use Bayesian Statistics?</h3>


<ul>
<li>Three points in its favor:

<ul>
<li>Universally applicable</li>
</ul></li>
</ul>

    </article>
	  <article class = "" id = "slide-34"> 
	    <h3>Why Use Bayesian Statistics?</h3>


<ul>
<li>Three points in its favor:

<ul>
<li>Universally applicable</li>
<li>Often more effective than maximum likelihood methods</li>
</ul></li>
</ul>

    </article>
	  <article class = "" id = "slide-35"> 
	    <h3>Why Use Bayesian Statistics?</h3>


<ul>
<li>Three points in its favor:

<ul>
<li>Universally applicable</li>
<li>Often more effective than maximum likelihood methods</li>
<li>Increasingly easy to apply to arbitrary problems in data analysis</li>
</ul></li>
</ul>

    </article>
	  <article class = "" id = "slide-36"> 
	    <h3>Why Not Use Bayesian Statistics?</h3>


<ul>
<li>Five points against it:

<ul>
<li>Can still be hard to apply to some problems</li>
</ul></li>
</ul>

    </article>
	  <article class = "" id = "slide-37"> 
	    <h3>Why Not Use Bayesian Statistics?</h3>


<ul>
<li>Five points against it:

<ul>
<li>Can still be hard to apply to some problems</li>
<li>Requires greater grasp of statistical theory to apply to new problems</li>
</ul></li>
</ul>

    </article>
	  <article class = "" id = "slide-38"> 
	    <h3>Why Not Use Bayesian Statistics?</h3>


<ul>
<li>Five points against it:

<ul>
<li>Can still be hard to apply to some problems</li>
<li>Requires greater grasp of statistical theory to apply to new problems</li>
<li>Rarely implemented in out-of-the-box software</li>
</ul></li>
</ul>

    </article>
	  <article class = "" id = "slide-39"> 
	    <h3>Why Not Use Bayesian Statistics?</h3>


<ul>
<li>Five points against it:

<ul>
<li>Can still be hard to apply to some problems</li>
<li>Requires greater grasp of statistical theory to apply to new problems</li>
<li>Rarely implemented in out-of-the-box software</li>
<li>Inaccurate prior knowledge can produce worse inferences than starting from scratch</li>
</ul></li>
</ul>

    </article>
	  <article class = "" id = "slide-40"> 
	    <h3>Why Not Use Bayesian Statistics?</h3>


<ul>
<li>Five points against it:

<ul>
<li>Can still be hard to apply to some problems</li>
<li>Requires greater grasp of statistical theory to apply to new problems</li>
<li>Rarely implemented in out-of-the-box software</li>
<li>Inaccurate prior knowledge can produce worse inferences than starting from scratch</li>
<li>Can be very slow</li>
</ul></li>
</ul>

    </article>
	  <article class = "" id = "slide-41"> 
	    <h3>What is Bayesian Nonparametrics?</h3>


<ul>
<li>We want to use prior knowledge while estimating flexible functional forms</li>
</ul>

    </article>
	  <article class = "" id = "slide-42"> 
	    <h3>What is Bayesian Nonparametrics?</h3>


<ul>
<li>We want to use prior knowledge while estimating flexible functional forms</li>
<li>Like frequentist methods, standard Bayesian models are parametric</li>
</ul>

    </article>
	  <article class = "" id = "slide-43"> 
	    <h3>What is Bayesian Nonparametrics?</h3>


<ul>
<li>We want to use prior knowledge while estimating flexible functional forms</li>
<li>Like frequentist methods, standard Bayesian models are parametric</li>
<li>Bayesian nonparametrics exploit a family of flexible distributions that encode prior knowledge</li>
</ul>

    </article>
	  <article class = "" id = "slide-44"> 
	    <h3>What is Bayesian Nonparametrics?</h3>


<ul>
<li>We want to use prior knowledge while estimating flexible functional forms</li>
<li>Like frequentist methods, standard Bayesian models are parametric</li>
<li>Bayesian nonparametrics exploit a family of flexible distributions that encode prior knowledge</li>
<li>The method has the capacity to use infinitely many parameters</li>
</ul>

    </article>
	  <article class = "" id = "slide-45"> 
	    <h3>What is Bayesian Nonparametrics?</h3>


<ul>
<li>We want to use prior knowledge while estimating flexible functional forms</li>
<li>Like frequentist methods, standard Bayesian models are parametric</li>
<li>Bayesian nonparametrics exploit a family of flexible distributions that encode prior knowledge</li>
<li>The method has the capacity to use infinitely many parameters</li>
<li>But prior knowledge encourages the use of a small number of parameters</li>
</ul>

    </article>
	  <article class = "" id = "slide-46"> 
	    <h3>What is Bayesian Nonparametrics?</h3>


<ul>
<li>Many statistical models require the <em>a priori</em> selection of integer parameters:

<ul>
<li><em>k</em>: Number of clusters in <em>k</em>-means</li>
<li><em>k</em>: Number of factors in factor analysis</li>
</ul></li>
</ul>

    </article>
	  <article class = "" id = "slide-47"> 
	    <h3>What is Bayesian Nonparametrics?</h3>


<ul>
<li>Many statistical models require the <em>a priori</em> selection of integer parameters:

<ul>
<li><em>k</em>: Number of clusters in <em>k</em>-means</li>
<li><em>k</em>: Number of factors in factor analysis</li>
</ul></li>
<li>Bayesian nonparametric methods use priors that allow us to infer these numbers from data</li>
</ul>

    </article>
	  <article class = "" id = "slide-48"> 
	    <h3>What is Bayesian Nonparametrics?</h3>


<ul>
<li>Many statistical models require the <em>a priori</em> selection of integer parameters:

<ul>
<li><em>k</em>: Number of clusters in <em>k</em>-means</li>
<li><em>k</em>: Number of factors in factor analysis</li>
</ul></li>
<li>Bayesian nonparametric methods use priors that allow us to infer these numbers from data</li>
<li>Prior information is still required, but data can influence the results -- unlike hardcoded parameters in traditional models</li>
</ul>

    </article>
	  <article class = "" id = "slide-49"> 
	    <h3>Core Tools in Bayesian Nonparametrics</h3>


<ul>
<li>Dirichlet Process

<ul>
<li>Distribution over distributions</li>
</ul></li>
</ul>

    </article>
	  <article class = "" id = "slide-50"> 
	    <h3>Core Tools in Bayesian Nonparametrics</h3>


<ul>
<li>Dirichlet Process

<ul>
<li>Distribution over distributions</li>
</ul></li>
<li>Chinese Restaurant Process

<ul>
<li>Distribution over category assignments</li>
</ul></li>
</ul>

    </article>
	  <article class = "" id = "slide-51"> 
	    <h3>Core Tools in Bayesian Nonparametrics</h3>


<ul>
<li>Dirichlet Process

<ul>
<li>Distribution over distributions</li>
</ul></li>
<li>Chinese Restaurant Process

<ul>
<li>Distribution over category assignments</li>
</ul></li>
<li>Indian Buffet Process

<ul>
<li>Distribution over active factors</li>
</ul></li>
</ul>

    </article>
	  <article class = "" id = "slide-52"> 
	    <h3>Chinese Restaurant Process</h3>


<p><img src="images/CRP.jpg" alt="CRP"></p>

    </article>
	  <article class = "" id = "slide-53"> 
	    <h3>Indian Buffet Process</h3>


<p><img src="images/IBP.jpg" alt="IBP"></p>

    </article>
	  <article class = "" id = "slide-54"> 
	    <h3>CRP vs. IBP Draws</h3>


<p><img src="images/IBP-CRP-Draws.jpg" alt="CRPvsIBP"></p>

    </article>
	  <article class = "" id = "slide-55"> 
	    <h3>Two Examples of Bayesian Nonparametrics</h3>


<ul>
<li>dp-Means: An easily implemented and fully automatic infinite cluster mixture model that borrows ideas from Bayesian nonparametrics</li>
</ul>

    </article>
	  <article class = "" id = "slide-56"> 
	    <h3>Two Examples of Bayesian Nonparametrics</h3>


<ul>
<li>dp-Means: An easily implemented and fully automatic infinite cluster mixture model that borrows ideas from Bayesian nonparametrics</li>
<li>dp-MM: A more flexible infinite cluster Gaussian Mixture model using a Dirichlet Process prior</li>
</ul>

    </article>
	  <article class = "" id = "slide-57"> 
	    <h3>dp-Means</h3>


<ul>
<li>The dp-Means algorithm is an analogue to k-means</li>
</ul>

    </article>
	  <article class = "" id = "slide-58"> 
	    <h3>dp-Means</h3>


<ul>
<li>The dp-Means algorithm is an analogue to k-means</li>
<li>It always converges and always reaches the same solution</li>
</ul>

    </article>
	  <article class = "" id = "slide-59"> 
	    <h3>dp-Means</h3>


<ul>
<li>The dp-Means algorithm is an analogue to k-means</li>
<li>It always converges and always reaches the same solution</li>
<li>It has one parameter: \(\lambda\)</li>
</ul>

    </article>
	  <article class = "" id = "slide-60"> 
	    <h3>dp-Means</h3>


<ul>
<li>The dp-Means algorithm is an analogue to k-means</li>
<li>It always converges and always reaches the same solution</li>
<li>It has one parameter: \(\lambda\)</li>
<li>If a point is further from the current clusters by more than \(\lambda\), it gets its own cluster</li>
</ul>

    </article>
	  <article class = "" id = "slide-61"> 
	    <h3>dp-Means</h3>


<pre><code>library(&quot;ggplot2&quot;)

source(&quot;dp-means.R&quot;)

data &lt;- generate.data()

ggplot(data, aes(x = x, y = y, color = assignment)) +
  geom_point()

dp.results &lt;- dp.means(data, 30)

ggplot(data, aes(x = x, y = y, color = dp.results$assignments)) +
  geom_point()
</code></pre>

    </article>
	  <article class = "" id = "slide-62"> 
	    <h3>dp-Means</h3>


<p><img src="../code/dp-means/0_dp-means.pdf" alt="dp-Means_0"></p>

    </article>
	  <article class = "" id = "slide-63"> 
	    <h3>dp-Means</h3>


<p><img src="../code/dp-means/1_dp-means.pdf" alt="dp-Means_1"></p>

    </article>
	  <article class = "" id = "slide-64"> 
	    <h3>dp-Means</h3>


<p><img src="../code/dp-means/10_dp-means.pdf" alt="dp-Means_10"></p>

    </article>
	  <article class = "" id = "slide-65"> 
	    <h3>dp-Means</h3>


<p><img src="../code/dp-means/50_dp-means.pdf" alt="dp-Means_50"></p>

    </article>
	  <article class = "" id = "slide-66"> 
	    <h3>dp-Means</h3>


<p><img src="../code/dp-means/100_dp-means.pdf" alt="dp-Means_100"></p>

    </article>
	  <article class = "" id = "slide-67"> 
	    <h3>dp-Means</h3>


<p><img src="../code/dp-means/125_dp-means.pdf" alt="dp-Means_125"></p>

    </article>
	  <article class = "" id = "slide-68"> 
	    <h3>dp-Means</h3>


<p><img src="../code/dp-means/200_dp-means.pdf" alt="dp-Means_200"></p>

    </article>
	  <article class = "" id = "slide-69"> 
	    <h3>dp-Means Algorithm</h3>


<pre><code>dp.means &lt;- function(data,
                     lambda = 1,
                     max.iterations = 100,
                     tolerance = 10e-3)
{
  n &lt;- nrow(data)
  k &lt;- 1
  assignments &lt;- rep(1, n)
  mu.x &lt;- mean(data$x)
  mu.y &lt;- mean(data$y)
</code></pre>

    </article>
	  <article class = "" id = "slide-70"> 
	    <h3>dp-Means Algorithm</h3>


<pre><code>  converged &lt;- FALSE
  iteration &lt;- 0

  ss.old &lt;- Inf
  ss.new &lt;- Inf

  while (!converged &amp;&amp; iteration &lt; max.iterations)
  {
    iteration &lt;- iteration + 1

    for (i in 1:n)
    {
      distances &lt;- rep(NA, k)

      for (j in 1:k)
      {
        distances[j] &lt;- (data[i, &#39;x&#39;] - mu.x[j])^2 + 
                        (data[i, &#39;y&#39;] - mu.y[j])^2
      }
</code></pre>

    </article>
	  <article class = "" id = "slide-71"> 
	    <h3>dp-Means Algorithm</h3>


<pre><code>      if (min(distances) &gt; lambda)
      {
        k &lt;- k + 1
        assignments[i] &lt;- k
        mu.x[k] &lt;- data[i, &#39;x&#39;]
        mu.y[k] &lt;- data[i, &#39;y&#39;]
      } else
      {
        assignments[i] &lt;- which(distances == min(distances))
      }
    }

    for (j in 1:k)
    {
      if (length(assignments == j) &gt; 0)
      {
        mu.x[j] &lt;- mean(data[assignments == j, &#39;x&#39;])
        mu.y[j] &lt;- mean(data[assignments == j, &#39;y&#39;])
      }
    }
</code></pre>

    </article>
	  <article class = "" id = "slide-72"> 
	    <h3>dp-Means Algorithm</h3>


<pre><code>    ss.new &lt;- 0

    for (i in 1:n)
    {
      ss.new &lt;- ss.new +
                (data[i, &#39;x&#39;] - mu.x[assignments[i]])^2 +
                (data[i, &#39;y&#39;] - mu.y[assignments[i]])^2
    }

    ss.change &lt;- ss.old - ss.new
    ss.old &lt;- ss.new

    if (!is.nan(ss.change) &amp;&amp; ss.change &lt; tolerance)
    {
      converged &lt;- TRUE
    }
  }

  centers &lt;- data.frame(x = mu.x, y = mu.y)
  return(list(&quot;centers&quot; = centers,
              &quot;assignments&quot; = factor(assignments),
              &quot;k&quot; = k,
              &quot;iterations&quot; = iteration))
}
</code></pre>

    </article>
	  <article class = "" id = "slide-73"> 
	    <h3>dp-Means</h3>


<ul>
<li>dp-Means is as useful as k-means, but it is not a proper Bayesian method</li>
</ul>

    </article>
	  <article class = "" id = "slide-74"> 
	    <h3>dp-Means</h3>


<ul>
<li>dp-Means is as useful as k-means, but it is not a proper Bayesian method</li>
<li>It cannot be used for online inference</li>
</ul>

    </article>
	  <article class = "" id = "slide-75"> 
	    <h3>dp-Means</h3>


<ul>
<li>dp-Means is as useful as k-means, but it is not a proper Bayesian method</li>
<li>It cannot be used for online inference</li>
<li>It cannot measure the uncertainty in its parameters</li>
</ul>

    </article>
	  <article class = "" id = "slide-76"> 
	    <h3>dp-MM</h3>


<ul>
<li>Proper Bayesian method</li>
</ul>

    </article>
	  <article class = "" id = "slide-77"> 
	    <h3>dp-MM</h3>


<ul>
<li>Proper Bayesian method</li>
<li>More flexible mixture modeling</li>
</ul>

    </article>
	  <article class = "" id = "slide-78"> 
	    <h3>dp-MM</h3>


<ul>
<li>Proper Bayesian method</li>
<li>More flexible mixture modeling</li>
<li>Infinitely many clusters</li>
</ul>

    </article>
	  <article class = "" id = "slide-79"> 
	    <h3>dp-MM</h3>


<ul>
<li>Proper Bayesian method</li>
<li>More flexible mixture modeling</li>
<li>Infinitely many clusters</li>
<li>Clusters can have different variances</li>
</ul>

    </article>
	  <article class = "" id = "slide-80"> 
	    <h3>dp-MM</h3>


<ul>
<li>Proper Bayesian method</li>
<li>More flexible mixture modeling</li>
<li>Infinitely many clusters</li>
<li>Clusters can have different variances</li>
<li>But there&#39;s no simple inference procedure</li>
</ul>

    </article>
	  <article class = "" id = "slide-81"> 
	    <h3>dp-MM</h3>


<ul>
<li>Proper Bayesian method</li>
<li>More flexible mixture modeling</li>
<li>Infinitely many clusters</li>
<li>Clusters can have different variances</li>
<li>But there&#39;s no simple inference procedure</li>
<li>Use MCMC</li>
</ul>

    </article>
	  <article class = "" id = "slide-82"> 
	    <h3>dp-MM</h3>


<pre><code>nIter &lt;- 100
results &lt;- dpmm_gibbs(x, 0.01, 0.1, 0.1, 0, 0.1, nIter)

ggplot(df, aes(x = X, fill = factor(results[, nIter]))) +
  geom_histogram(binwidth = 3) +
  opts(legend.position = &quot;none&quot;) +
  opts(title = &quot;dp-MM with alpha = 0.01&quot;)
ggsave(&quot;code/dpmm/dpmm_0.01.pdf&quot;, height = 7, width = 7)
</code></pre>

    </article>
	  <article class = "" id = "slide-83"> 
	    <h3>dp-MM</h3>


<p><img src="../code/dpmm/dpmm_0.5.pdf" alt="dpMM_0.5"></p>

    </article>
	  <article class = "" id = "slide-84"> 
	    <h3>dp-MM</h3>


<p><img src="../code/dpmm/dpmm_2.5.pdf" alt="dpMM_2.5"></p>

    </article>
	  <article class = "" id = "slide-85"> 
	    <h3>dp-MM</h3>


<p><img src="../code/dpmm/dpmm_12.5.pdf" alt="dpMM_12.5"></p>

    </article>
	  <article class = "" id = "slide-86"> 
	    <h3>dp-MM</h3>


<p><img src="../code/dpmm/dpmm_100.0.pdf" alt="dpMM_100.0"></p>

    </article>
	  <article class = "" id = "slide-87"> 
	    <h3>The Near Future of Bayesian Nonparametrics</h3>


<ul>
<li>Variational inference</li>
</ul>

    </article>
	  <article class = "" id = "slide-88"> 
	    <h3>The Near Future of Bayesian Nonparametrics</h3>


<ul>
<li>Variational inference</li>
<li>Stochastic variational inference</li>
</ul>

    </article>
	  <article class = "" id = "slide-89"> 
	    <h3>The Near Future of Bayesian Nonparametrics</h3>


<ul>
<li>Variational inference</li>
<li>Stochastic variational inference</li>
<li>Applying Bayesian nonparametrics to huge, streaming data sets</li>
</ul>

    </article>
	  <article class = "" id = "slide-90"> 
	    <h3>References</h3>


<ul>
<li><a href="http://www.cs.princeton.edu/%7Eblei/papers/GershmanBlei2012.pdf">Bayesian Nonparametrics Tutorial</a></li>
<li><a href="http://arxiv.org/pdf/1111.0352.pdf">dp-Means ArXiv Paper</a></li>
<li><a href="http://www.cs.princeton.edu/courses/archive/fall07/cos597C/scribe/20070921.pdf">Chinese Restaurant Process</a></li>
<li><a href="http://jmlr.csail.mit.edu/papers/volume12/griffiths11a/griffiths11a.pdf">Indian Buffet Process</a></li>
<li><a href="http://arxiv.org/pdf/1206.7051v1.pdf">Stochastic Variational Inference</a></li>
</ul>

    </article>
  </section>
</body>
  <!-- LOAD JAVASCRIPTS  -->
	<script src='/Library/Frameworks/R.framework/Versions/2.15/Resources/library/slidify/libraries/html5slides/default/slides.js'></script>
	<!-- LOAD MATHJAX JS -->
  <script type="text/x-mathjax-config">
     MathJax.Hub.Config({
       tex2jax: {
         inlineMath: [['$','$'], ['\\(','\\)']],
         processEscapes: true
       }
     });
  </script>
  <script type="text/javascript"  
src="https://c328740.ssl.cf1.rackcdn.com/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
  <!-- DONE LOADING MATHJAX -->
	  <!-- LOAD HIGHLIGHTER JS FILES -->
  <script src="/Library/Frameworks/R.framework/Versions/2.15/Resources/library/slidify/libraries/highlight.js/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <!-- DONE LOADING CSS FILES AND JS -->

		
	
</html>

